{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "799e28cb",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14ec93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cctx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d # For 3D plots if needed\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score,davies_bouldin_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddbd6fe",
   "metadata": {},
   "source": [
    "Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11c13c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_crypto_data(tickers,timeframe=\"1d\",limit=365):\n",
    "    \"\"\" Fetch OHLCV data for a list of tickers from Binance\n",
    "        Args:\n",
    "        tickers (list): List of ticker symbols (e.g., ['BTC/USDT', 'ETH/USDT']).\n",
    "        timeframe (str): The timeframe for candles (e.g., '1d', '4h', '1h').\n",
    "        limit (int): The number of candles to fetch.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are tickers and values are pandas DataFrames\n",
    "              of their OHLCV data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Starting data fetch for {len(tickers)} tickers......\")\n",
    "    # Initialize the binance exchange interface\n",
    "    exchange = cctx.binance()\n",
    "\n",
    "    # Standard columns names for OHLCV data\n",
    "    columns = [\"timestamp\",\"open\",\"high\",\"low\",\"close\",\"volume\"]\n",
    "\n",
    "    all_data = {}\n",
    "\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Fetch the data\n",
    "            # cctx returns a list of lists\n",
    "            # [[timestamp,open,high,low,close,volume]]\n",
    "            ohlcv = exchange.fetch_ohlcv(ticker,timeframe,limit=limit)\n",
    "\n",
    "            # Convert the list of lists to a pandas DataFame\n",
    "            df = pd.DataFrame(ohlcv,columns=columns)\n",
    "\n",
    "            # Convert timestamp (milliseconds) to a readable datatime format\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],unit=\"ms\")\n",
    "\n",
    "            # Store the DataFrame in our dictionary\n",
    "            all_data[ticker] = df\n",
    "            print(f\"Successfully fetched {ticker}\")\n",
    "\n",
    "            # Be polite to the API: wait a bit before the next request\n",
    "            # This helps avoid rate limits\n",
    "            time.sleep(exchange.ratelimit / 1000) # (ratelimit is in ms)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch data for {ticker}: {e}\")\n",
    "\n",
    "    print(\"Data fetch complete\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60caee7e",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee9ac5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(data_dict):\n",
    "    \"\"\"\n",
    "    Engineers features for each crypto to be used for clustering.\n",
    "    We are clustering the *cryptocurrencies themselves*, so we need\n",
    "    to aggregate the time-series data into a single row of features\n",
    "    for each crypto.\n",
    "    \n",
    "    Args:\n",
    "        data_dict (dict): The dictionary of DataFrames from fetch_crypto_data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame where each row is a crypto and\n",
    "                      each column is an engineered feature.\n",
    "    \"\"\"\n",
    "    print(\"Engineering features\")\n",
    "    features = []\n",
    "\n",
    "    for ticker,df in data_dict.items():\n",
    "        if df.empty:\n",
    "            continue\n",
    "        # 1. Daily Returns: (Close - Open) / Open\n",
    "        # We calculate the average daily return\n",
    "        daily_return = (df[\"close\"] - df[\"open\"]) / df[\"open\"]\n",
    "        avg_daily_return = daily_return.std()\n",
    "\n",
    "        # 2. Average Daily Volume (in terms of the quote currency, eg USDT)\n",
    "        # We approximate this by (close * volume)\n",
    "        avg_daily_volume = (df[\"close\"] * df[\"volume\"]).mean()\n",
    "\n",
    "        # 3. Volatility: Standard deviation of daily returns\n",
    "        # This measures how risky or unstable the asset is\n",
    "        volatility = daily_return.std()\n",
    "\n",
    "        # 4. Average Daily Range:(High - Low) / Close\n",
    "        # This measures the average intraday price swing\n",
    "        daily_range = (df[\"high\"] - df[\"low\"]) / df[\"close\"]\n",
    "        avg_daily_range = daily_range.mean()\n",
    "\n",
    "        # Append the features for this ticker to our list\n",
    "        features.append({\n",
    "            \"ticker\":ticker,\n",
    "            \"avg_daily_return\":avg_daily_return,\n",
    "            \"volatility\":volatility,\n",
    "            \"avg_daily_volume\":avg_daily_volume,\n",
    "            \"avg_daily_range\":avg_daily_range\n",
    "        })\n",
    "\n",
    "\n",
    "    # Convert the lsit of dictionaries to a DataFrame\n",
    "    feature_df = pd.DataFrame(features)\n",
    "\n",
    "    # Set the ticker as the index\n",
    "    feature_df = feature_df.set_index(\"ticker\")\n",
    "\n",
    "    # Drop any rows with missing data (e.g, if a crypto had no volume)\n",
    "    feature_df = feature_df.dropna()\n",
    "\n",
    "    print(\"Feature engineering complete\")\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131f7272",
   "metadata": {},
   "source": [
    "Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1eb0564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Scales the feature DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The feature-engineered DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (scaled_data, scaler_object)\n",
    "               The scaled data (numpy array) and the scaler\n",
    "               object (to transform new data later).\n",
    "    \"\"\"\n",
    "    print(\"Scaling data..........\")\n",
    "    # Clustering algorithms (K-Means,DBSCAN) are distance-based\n",
    "    # This means features large scales (like \"avg_daily_volume\")\n",
    "    # will dominate features with small scales (like \"avg_daily_return\")\n",
    "    # We MUST scale the data. StandardScaler transform data to have\n",
    "    # a mean of 0 and standard deviation of 1\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # .fit_transform() calculates the mean/std and applies the scaling\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "    return scaled_data,scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09deaaf4",
   "metadata": {},
   "source": [
    "Pre-Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "915f8671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_before(scaled_data,feature_df):\n",
    "    # Visualizes the dataset before clustering using PCA\n",
    "    # PCA (principal component analysis) reduces our 4 features\n",
    "    # down to 2, allowing us to plot them on a 2D scatter plot\n",
    "\n",
    "    print(\"Visualizing data before clustering (using PCA).......\")\n",
    "\n",
    "    # Initialize PCA to reduce to 2 components (for 2D plotting)\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    # Fot and transform the scaled data\n",
    "    data_pca = pca.fit_transform(scaled_data)\n",
    "\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.scatter(data_pca[:,0],data_pca[:,1])\n",
    "\n",
    "    # Add labels for each point\n",
    "    for i,ticker in enumerate(feature_df.index):\n",
    "        plt.annotate(ticker,(data_pca[i,0],data_pca[i,1]),\n",
    "                     textcoords=\"offset points\",xytext=(0,5),ha=\"center\",fontsize=8)\n",
    "        \n",
    "    plt.title(\"Cryptocurrency Data (Before Clustering) - PCA View\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Return the PCA object and data for later\n",
    "    return pca,data_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bcea84",
   "metadata": {},
   "source": [
    "K-Means Clustering (Centriod-Based)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
