{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "799e28cb",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14ec93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cctx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d # For 3D plots if needed\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score,davies_bouldin_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddbd6fe",
   "metadata": {},
   "source": [
    "Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11c13c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_crypto_data(tickers,timeframe=\"1d\",limit=365):\n",
    "    \"\"\" Fetch OHLCV data for a list of tickers from Binance\n",
    "        Args:\n",
    "        tickers (list): List of ticker symbols (e.g., ['BTC/USDT', 'ETH/USDT']).\n",
    "        timeframe (str): The timeframe for candles (e.g., '1d', '4h', '1h').\n",
    "        limit (int): The number of candles to fetch.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are tickers and values are pandas DataFrames\n",
    "              of their OHLCV data.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Starting data fetch for {len(tickers)} tickers......\")\n",
    "    # Initialize the binance exchange interface\n",
    "    exchange = cctx.binance()\n",
    "\n",
    "    # Standard columns names for OHLCV data\n",
    "    columns = [\"timestamp\",\"open\",\"high\",\"low\",\"close\",\"volume\"]\n",
    "\n",
    "    all_data = {}\n",
    "\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Fetch the data\n",
    "            # cctx returns a list of lists\n",
    "            # [[timestamp,open,high,low,close,volume]]\n",
    "            ohlcv = exchange.fetch_ohlcv(ticker,timeframe,limit=limit)\n",
    "\n",
    "            # Convert the list of lists to a pandas DataFame\n",
    "            df = pd.DataFrame(ohlcv,columns=columns)\n",
    "\n",
    "            # Convert timestamp (milliseconds) to a readable datatime format\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],unit=\"ms\")\n",
    "\n",
    "            # Store the DataFrame in our dictionary\n",
    "            all_data[ticker] = df\n",
    "            print(f\"Successfully fetched {ticker}\")\n",
    "\n",
    "            # Be polite to the API: wait a bit before the next request\n",
    "            # This helps avoid rate limits\n",
    "            time.sleep(exchange.ratelimit / 1000) # (ratelimit is in ms)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not fetch data for {ticker}: {e}\")\n",
    "\n",
    "    print(\"Data fetch complete\")\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60caee7e",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee9ac5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(data_dict):\n",
    "    \"\"\"\n",
    "    Engineers features for each crypto to be used for clustering.\n",
    "    We are clustering the *cryptocurrencies themselves*, so we need\n",
    "    to aggregate the time-series data into a single row of features\n",
    "    for each crypto.\n",
    "    \n",
    "    Args:\n",
    "        data_dict (dict): The dictionary of DataFrames from fetch_crypto_data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame where each row is a crypto and\n",
    "                      each column is an engineered feature.\n",
    "    \"\"\"\n",
    "    print(\"Engineering features\")\n",
    "    features = []\n",
    "\n",
    "    for ticker,df in data_dict.items():\n",
    "        if df.empty:\n",
    "            continue\n",
    "        # 1. Daily Returns: (Close - Open) / Open\n",
    "        # We calculate the average daily return\n",
    "        daily_return = (df[\"close\"] - df[\"open\"]) / df[\"open\"]\n",
    "        avg_daily_return = daily_return.std()\n",
    "\n",
    "        # 2. Average Daily Volume (in terms of the quote currency, eg USDT)\n",
    "        # We approximate this by (close * volume)\n",
    "        avg_daily_volume = (df[\"close\"] * df[\"volume\"]).mean()\n",
    "\n",
    "        # 3. Volatility: Standard deviation of daily returns\n",
    "        # This measures how risky or unstable the asset is\n",
    "        volatility = daily_return.std()\n",
    "\n",
    "        # 4. Average Daily Range:(High - Low) / Close\n",
    "        # This measures the average intraday price swing\n",
    "        daily_range = (df[\"high\"] - df[\"low\"]) / df[\"close\"]\n",
    "        avg_daily_range = daily_range.mean()\n",
    "\n",
    "        # Append the features for this ticker to our list\n",
    "        features.append({\n",
    "            \"ticker\":ticker,\n",
    "            \"avg_daily_return\":avg_daily_return,\n",
    "            \"volatility\":volatility,\n",
    "            \"avg_daily_volume\":avg_daily_volume,\n",
    "            \"avg_daily_range\":avg_daily_range\n",
    "        })\n",
    "\n",
    "\n",
    "    # Convert the lsit of dictionaries to a DataFrame\n",
    "    feature_df = pd.DataFrame(features)\n",
    "\n",
    "    # Set the ticker as the index\n",
    "    feature_df = feature_df.set_index(\"ticker\")\n",
    "\n",
    "    # Drop any rows with missing data (e.g, if a crypto had no volume)\n",
    "    feature_df = feature_df.dropna()\n",
    "\n",
    "    print(\"Feature engineering complete\")\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131f7272",
   "metadata": {},
   "source": [
    "Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1eb0564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Scales the feature DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The feature-engineered DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (scaled_data, scaler_object)\n",
    "               The scaled data (numpy array) and the scaler\n",
    "               object (to transform new data later).\n",
    "    \"\"\"\n",
    "    print(\"Scaling data..........\")\n",
    "    # Clustering algorithms (K-Means,DBSCAN) are distance-based\n",
    "    # This means features large scales (like \"avg_daily_volume\")\n",
    "    # will dominate features with small scales (like \"avg_daily_return\")\n",
    "    # We MUST scale the data. StandardScaler transform data to have\n",
    "    # a mean of 0 and standard deviation of 1\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # .fit_transform() calculates the mean/std and applies the scaling\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "    return scaled_data,scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09deaaf4",
   "metadata": {},
   "source": [
    "Pre-Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "915f8671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_before(scaled_data,feature_df):\n",
    "    # Visualizes the dataset before clustering using PCA\n",
    "    # PCA (principal component analysis) reduces our 4 features\n",
    "    # down to 2, allowing us to plot them on a 2D scatter plot\n",
    "\n",
    "    print(\"Visualizing data before clustering (using PCA).......\")\n",
    "\n",
    "    # Initialize PCA to reduce to 2 components (for 2D plotting)\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    # Fot and transform the scaled data\n",
    "    data_pca = pca.fit_transform(scaled_data)\n",
    "\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.scatter(data_pca[:,0],data_pca[:,1])\n",
    "\n",
    "    # Add labels for each point\n",
    "    for i,ticker in enumerate(feature_df.index):\n",
    "        plt.annotate(ticker,(data_pca[i,0],data_pca[i,1]),\n",
    "                     textcoords=\"offset points\",xytext=(0,5),ha=\"center\",fontsize=8)\n",
    "        \n",
    "    plt.title(\"Cryptocurrency Data (Before Clustering) - PCA View\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Return the PCA object and data for later\n",
    "    return pca,data_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bcea84",
   "metadata": {},
   "source": [
    "K-Means Clustering (Centriod-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d11292e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_run_kmeans(scaled_data):\n",
    "    # Finds the optimal \"k\" and runs K-Means Clustering\n",
    "\n",
    "    print(\"----- K-Means Clustering -----\")\n",
    "\n",
    "    # Hyperparameter Tuning: Finding the best \"k\"\n",
    "\n",
    "    # Method 1: The Elbow Method\n",
    "    # We plot the \"Within-Cluster\" Sum of Squares (WCSS) for \n",
    "    # different values of k. The \"elbow\" is the point where\n",
    "    # WCSS starts to decrease less dramatically\n",
    "    wcss = []\n",
    "    k_range = range(1,11) # Test k from 1 to 10\n",
    "\n",
    "\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k,init=\"k-means++\",random_state=42,n_init=10)\n",
    "        kmeans.fit(scaled_data)\n",
    "        wcss.append(kmeans.inertia_) # inertia_ is the WCSS\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.Subplot(1,2,1)\n",
    "    plt.plot(k_range,wcss,\"bo-\")\n",
    "    plt.title(\"K-Means Elbow Method\")\n",
    "    plt.xlabel(\"Number of Clusters (k)\")\n",
    "    plt.ylabel(\"WCSS (Inertia)\")\n",
    "\n",
    "\n",
    "    # Method 2: Silhouette Analysis\n",
    "    # The Silhouette Score measures how similar a point is to its\n",
    "    # own cluster compared to other clusters\n",
    "    # Score ranges from -1 (bad) to +1 (good)\n",
    "    silhouette_scores = []\n",
    "    k_range_sil = range(2,11) # Silhouette score needs at least 2 clusters\n",
    "\n",
    "\n",
    "    for k in k_range_sil:\n",
    "        kmeans = KMeans(n_clusters=k,init=\"k-means++\",random_state=42,n_init=10)\n",
    "        kmeans.fit(scaled_data)\n",
    "        score = silhouette_score(scaled_data,kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "\n",
    "    plt.Subplot(1,2,2)\n",
    "    plt.plot(k_range_sil,silhouette_scores,\"rs-\")\n",
    "    plt.title(\"K-Means Silhoutte Scores\")\n",
    "    plt.xlabel(\"Number of Clusters (k)\")\n",
    "    plt.ylabel(\"Average Silhoutte Score\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    # Analysis\n",
    "    # Based on the plot, we choose the best \"k\"\n",
    "    # For the elbow, it's often 3 or 4\n",
    "    # For Silhouette, we pick the highest score\n",
    "    # Let's assume the plots suggest k=4 is a good balance\n",
    "    optimal_k = 3 # This should be chosen based on your plots\n",
    "    print(f\"Based on analysis (Elbow & Silhouette), choosing k = {optimal_k}\")\n",
    "\n",
    "\n",
    "    # ----- Final Model Training -----\n",
    "    kmeans_model = KMeans(n_clusters=optimal_k,init=\"k-means++\",random_state=42,n_init=10)\n",
    "    kmeans_model.fit(scaled_data)\n",
    "\n",
    "    # Get the cluster labels for each data point\n",
    "    kmeans_labels = kmeans_model.labels_\n",
    "\n",
    "    # Evaluate the final model\n",
    "    sil_score = silhouette_score(scaled_data,kmeans_labels)\n",
    "    db_score = davies_bouldin_score(scaled_data,kmeans_labels)\n",
    "\n",
    "    print(f\"K-Means (k={optimal_k}) Silhouette Score: {sil_score:.4f}\")\n",
    "    print(f\"K-Means (k={optimal_k}) Davies-Bouldin Score: {db_score:.4f}\")\n",
    "\n",
    "\n",
    "    return kmeans_model,kmeans_labels,(sil_score,db_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e66cf",
   "metadata": {},
   "source": [
    "Agglomerative Clustering (Hierarchial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "320b6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram,linkage\n",
    "\n",
    "def tune_and_run_agglo(scaled_data):\n",
    "    # Visualizes the dendogram and runs Agglomerative Clustering\n",
    "\n",
    "    print(\"-----  Agglomerative Clustering -----\")\n",
    "\n",
    "    # Hyperparameter Tuning: The Dendrogram\n",
    "    # The dendrogram is a tree diagram that shows the hierarchical\n",
    "    # relationships. We \"tune\" it by visually inspecting it\n",
    "    # to find the best place to \"cut\" the tree\n",
    "\n",
    "    # \"ward\" linkage minimizes the variance of the clusters being merged\n",
    "    linked = linkage(scaled_data,method=\"ward\")\n",
    "\n",
    "    plt.figure(figsize=(12,7))\n",
    "    dendrogram(linked,\n",
    "               orientation=\"top\",\n",
    "               labels=feature_df.index,\n",
    "               distance_sort=\"descending\",\n",
    "               show_leaf_counts=True)\n",
    "    plt.title(\"Agglomerative Clustering Dendrogram\")\n",
    "    plt.xlabel(\"Cryptocurrency\")\n",
    "    plt.ylabel(\"Distance (Ward)\")\n",
    "    plt.axhline(y=3.5,color=\"r\",linestyle=\"'--\") # Add a line to show a potential cut\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # ----- Analysis -----\n",
    "    # Look for the longest vertical lines that are not cut by\n",
    "    # a horizontal line. Cutting at y=3.5 (red line) would\n",
    "    # result in 4 clsuters Let's use n_clusters=4\n",
    "    optimal_n = 4 # This should be chosen based on the denndrogram\n",
    "    print(f\"Based on dendrogram, choosing n_clusters = {optimal_n}\")\n",
    "\n",
    "    # ----- Final Model Training -----\n",
    "    agglo_model = AgglomerativeClustering(n_clusters=optimal_n,linkage=\"ward\")\n",
    "\n",
    "    # .fit_predict() trains and returns the labels\n",
    "    agglo_labels = agglo_model.fit_predict(scaled_data)\n",
    "\n",
    "    # Evaluate the final model\n",
    "    sil_score = silhouette_score(scaled_data,agglo_labels)\n",
    "    db_score = davies_bouldin_score(scaled_data,agglo_labels)\n",
    "\n",
    "    print(f\"Agglomerative (n={optimal_n}) Silhouette Score: {sil_score:.4f}\")\n",
    "    print(f\"Agglomerative (n={optimal_n}) Davies-Bouldin Score: {db_score:.4f}\")\n",
    "\n",
    "\n",
    "    return agglo_model,agglo_labels,(sil_score,db_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a505bb",
   "metadata": {},
   "source": [
    "DBSCAN Clustering (Density-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0df64a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_run_dbscan(scaled_data):\n",
    "    # Finds optimal \"eps\" and runs DBSCAN\n",
    "    print(\"----- DBSCAN Clustering -----\")\n",
    "\n",
    "    # Hyperparameter Tuning: Finding \"eps\" and min_samples\n",
    "\n",
    "    # 1. \"min_samples\" : A good rule of thumb is (2 * num_features)\n",
    "    # We have 4 features, so min_samples = 8 is a good start\n",
    "    min_samples = 2 * scaled_data.shape[1]\n",
    "\n",
    "    # 2. \"eps\": To find \"eps\", we plot the distance of each point to\n",
    "    # its k-th nearest neighbor (where k = min_samples)\n",
    "    # We look for the \"knee\" or \"elbow\" in the plot\n",
    "\n",
    "    # Find the distance to the min_samples-th neighbor\n",
    "    neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "    nbrs = neighbors.fit(scaled_data)\n",
    "    distances,indices = nbrs.kneighbors(scaled_data)\n",
    "\n",
    "    # Get the distance to  the k-th neigbour (index min_samples-1)\n",
    "    # and sort them\n",
    "    k_distances = np.sort(distances[:,min_samples-1],axis=0)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(k_distances)\n",
    "    plt.title(f\"K-Distance Plot  (k={min_samples})\")\n",
    "    plt.xlabel(\"Points (sorted by distance)\")\n",
    "    plt.ylabel(f\"{min_samples}-th Neighbor Distance (eps)\")\n",
    "    plt.axhline(y=1.5,color=\"r\",linestyle=\"--\") # Add a line for the \"knee\"\n",
    "    plt.show()\n",
    "\n",
    "    # ----- Analysis -----\n",
    "    # The plot shows a sharp bend (knee) around y=1.5\n",
    "    # This suggests a good value for \"eps\"\n",
    "    optimal_eps = 1.5 # This should be chosen based on the plot\n",
    "    print(f\"Based on k-distance plot, choosing eps = {optimal_eps}\")\n",
    "    print(f\"Using min_samples = {min_samples}\")\n",
    "\n",
    "    # ----- Final Model Training -----\n",
    "    dbscan_model = DBSCAN(eps=optimal_eps,min_samples=min_samples)\n",
    "    dbscan_labels = dbscan_model.fit_predict(scaled_data)\n",
    "\n",
    "    # DBSCAN labels:\n",
    "    # -1: Noise (Outlier)\n",
    "    # 0,1,2, ........ Cluster labels\n",
    "    n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "    n_noise = list(dbscan_labels).count(-1)\n",
    "\n",
    "    print(f\"DBSCAN found {n_clusters} clusters and {n_noise} noise points\")\n",
    "\n",
    "    # ----- Evaluation -----\n",
    "    # IMPORTANT: We must exclude noise points (-1)  when calculating\n",
    "    # metrics like Silhouette,as they don't belong to a cluster\n",
    "\n",
    "    # Check if more than one cluster was found (excluding noise)\n",
    "    if n_clusters > 1:\n",
    "        # Create a mask to select only non-noise points\n",
    "        non_noise_mask = (dbscan_labels != -1)\n",
    "\n",
    "        # Selects the data and labels for non-noise points\n",
    "        data_filtered = scaled_data[non_noise_mask]\n",
    "        labels_filtered = dbscan_labels[non_noise_mask]\n",
    "\n",
    "        sil_score = silhouette_score(data_filtered,labels_filtered)\n",
    "        db_score = davies_bouldin_score(data_filtered,labels_filtered)\n",
    "\n",
    "        print(f\"DBSCAN (non-noise) Silhoette Score: {sil_score:.4f}\")\n",
    "        print(f\"DBSCAN (non-noise) Davies-Bouldin Score: {db_score:.4f}\")\n",
    "\n",
    "    else:\n",
    "        print(\"DBSCAN did not find more than one cluster. Metrics cannot be calculated\")\n",
    "        sil_score,db_score = np.nan, np.nan\n",
    "\n",
    "\n",
    "    return dbscan_model,dbscan_labels, (sil_score,db_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3e6b2",
   "metadata": {},
   "source": [
    "Post-Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cc5f6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparison(data_pca,labels_dict,metrics_dict,feature_df):\n",
    "    # Create a side-by-side plot comparing the clustering results\n",
    "    print(\"\\nVisualizing Clustering Comparison........\")\n",
    "\n",
    "    fig,axes = plt.subplots(1,3,figsize=(24,8),sharex=True,sharey=True)\n",
    "    plt.suptitle(\"Clustering Model Comparison (on 2D  PCA Data)\",fontsize=20,y=1.02)\n",
    "\n",
    "    plot_map = {\n",
    "        \"K-Means\":labels_dict[\"Kmeans\"],\n",
    "        \"Agglomerative\":labels_dict[\"agglo\"],\n",
    "        \"DBSCAN\":labels_dict[\"dbscan\"]\n",
    "    }\n",
    "\n",
    "\n",
    "    for i, (title,labels) in enumerate(plot_map.items()):\n",
    "        ax = axes[1]\n",
    "\n",
    "        # Use a consistent color palette\n",
    "        # Create a palette that includes -1 (for noise) if present\n",
    "        unique_labels = sorted(list(set(labels)))\n",
    "        palette = sns.color_palette(\"deep\",len(unique_labels))\n",
    "\n",
    "        # Make noise points (-1) black\n",
    "        color_map = {label: color for label,color in zip(unique_labels,palette)}\n",
    "        if -1 in color_map:\n",
    "            color_map[-1] = (0,0,0) # Black for noise\n",
    "\n",
    "        # Map labels to colors\n",
    "        colors = [color_map[label] for label in labels]\n",
    "\n",
    "        ax.scatter(data_pca[:,0], data_pca[:,1], c=colors)\n",
    "\n",
    "        # Add metrics to the title\n",
    "        sil = metrics_dict[title][0]\n",
    "        db = metrics_dict[title][1]\n",
    "        ax.set_title(f\"{title}\",\n",
    "                     f\"Silhouette:{sil:.4f} (Higher is better)\",\n",
    "                     f\"Davies-Bouldin: {db:.4f} (Lower is better)\",\n",
    "                     fontsize=14)\n",
    "        ax.set_ylabel(\"Principal Component 1\")\n",
    "    axes[0].set_ylabel(\"Principal Component 2\")\n",
    "\n",
    "    # Add labels (can get crowded, so we do it once)\n",
    "    for i, ticker in enumerate(feature_df.index):\n",
    "        axes[1].annotate(ticker,(data_pca[i,0],data_pca[i,1]),\n",
    "                         textcoords=\"offset points\",xytext=(0.5),\n",
    "                         ha=\"center\",fontsize=8,color=\"gray\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # ----- Print Final Report -----\n",
    "    print(\"----- Final Model Comparison -----\")\n",
    "    print(\"Silhouette Score (Higher is better, range -1 to 1):\")\n",
    "    for model,(sil,db) in metrics_dict.items():\n",
    "        print(f\"    - {model}: {db:.4f}\")\n",
    "\n",
    "\n",
    "    print(\"Davies Bouldin Score (Lower is better, min 0):\")\n",
    "    for model, (sil,db) in metrics_dict.items():\n",
    "        print(f\"   - {model}: {db:.4f}\")\n",
    "\n",
    "\n",
    "    # ----- Which is best ? ------\n",
    "    # This is subjective!\n",
    "    # K-Means and Agglomerative force all points into a cluster\n",
    "    # DBSCAN is great for outlier detectio (the noise points)\n",
    "\n",
    "    # We can programmatically find the \"best\" based on metrics\n",
    "    best_sil = max(metrics_dict.items(),key=lambda item: item[1][0])\n",
    "    best_db = min(metrics_dict.items(),key=lambda item: item[1][1])\n",
    "\n",
    "    print(\"----- Conclusion -----\")\n",
    "    print(f\"Best by Silhouette Score: {best_sil[0]} ({best_db[1][0]:.4f})\")\n",
    "    print(f\"Best by Davies-Bouldin Score: {best_db[0]} ({best_db[1][1]:.4f})\")\n",
    "\n",
    "\n",
    "    if best_sil[0] == best_db[0]:\n",
    "        print(f\"Overall, {best_sil[0]} appears to be strongest model quantitatively\")\n",
    "    else:\n",
    "        print(f\"The metrics disagree. {best_sil[0]} is best by separation, but {best_db[0]}\")\n",
    "\n",
    "\n",
    "    print(\"Qualitative Analysis\")\n",
    "    print(\"K-Means/Agglomerative: Good for segmenting all assets into groups (e.g, high-risk,stable)\")\n",
    "    print(\"DBSCAN: Good for finding normal assets and outliers (the noise points)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b269c",
   "metadata": {},
   "source": [
    "New Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20b8fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_crypto(new_ohlcv_data, scaler, kmeans_model):\n",
    "    \"\"\"\n",
    "    Predicts the cluster for a new, unseen cryptocurrency.\n",
    "    We will use the K-Means model as it's the most common\n",
    "    for new predictions.\n",
    "    \n",
    "    Args:\n",
    "        new_ohlcv_data (list): A list of [timestamp, o, h, l, c, v]\n",
    "                               lists, just like ccxt returns.\n",
    "        scaler (StandardScaler): The *original* scaler object\n",
    "                                 we saved from preprocessing.\n",
    "        kmeans_model (KMeans): The *original* trained K-Means model.\n",
    "\n",
    "    Returns:\n",
    "        int: The predicted cluster label.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"----- New Prediction -----\")\n",
    "    # 1. Convert to DataFrame\n",
    "    df = pd.DataFrame(new_ohlcv_data,columns=[\"timestamp\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
    "\n",
    "    # 2. Wrap in a dictioanry for the feature engineer\n",
    "    data_dict = {\"NEW CRYPTO\":df}\n",
    "\n",
    "    # 3. Engineer Features\n",
    "    # This will reeturn a 1-row DataFrame\n",
    "    feature_df = engineer_features(data_dict)\n",
    "\n",
    "    # 4. Scale the Data\n",
    "    # We use .transform(), NOT .fit_transform()!\n",
    "    # We must apply the same scaling as the original training data\n",
    "    scaled_feature = scaler.transform(feature_df)\n",
    "\n",
    "    print(f\"New Crypto Features (Scaled): {scaled_feature}\")\n",
    "\n",
    "    # 5. Predict\n",
    "    cluster = kmeans_model.predict(scaled_feature)\n",
    "\n",
    "\n",
    "    return cluster[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a18347",
   "metadata": {},
   "source": [
    "Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59174740",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Define Tickers ---\n",
    "    # We want a good mix of assets\n",
    "    crypto_tickers = [\n",
    "        'BTC/USDT', 'ETH/USDT', 'SOL/USDT', 'DOGE/USDT', 'LINK/USDT',\n",
    "        'MATIC/USDT', 'XRP/USDT', 'LTC/USDT', 'ADA/USDT', 'AVAX/USDT',\n",
    "        'BNB/USDT', 'TRX/USDT', 'DOT/USDT', 'UNI/USDT', 'SHIB/USDT'\n",
    "    ]\n",
    "    \n",
    "    # --- Step 1 & 2 ---\n",
    "    raw_data_dict = fetch_crypto_data(crypto_tickers)\n",
    "    feature_df = engineer_features(raw_data_dict)\n",
    "    \n",
    "    # Check the engineered features\n",
    "    print(\"\\n--- Engineered Features ---\")\n",
    "    print(feature_df.head())\n",
    "    \n",
    "    # --- Step 3 ---\n",
    "    scaled_data, data_scaler = preprocess_data(feature_df)\n",
    "    \n",
    "    # --- Step 4 ---\n",
    "    pca_obj, data_pca = visualize_before(scaled_data, feature_df)\n",
    "    \n",
    "    # --- Step 5, 6, 7 ---\n",
    "    # We will store the models, labels, and metrics for comparison\n",
    "    models = {}\n",
    "    all_labels = {}\n",
    "    all_metrics = {}\n",
    "    \n",
    "    # K-Means\n",
    "    models['kmeans'], all_labels['kmeans'], all_metrics['K-Means'] = \\\n",
    "        tune_and_run_kmeans(scaled_data)\n",
    "        \n",
    "    # Agglomerative\n",
    "    models['agglo'], all_labels['agglo'], all_metrics['Agglomerative'] = \\\n",
    "        tune_and_run_agglo(scaled_data)\n",
    "\n",
    "    # DBSCAN\n",
    "    models['dbscan'], all_labels['dbscan'], all_metrics['DBSCAN'] = \\\n",
    "        tune_and_run_dbscan(scaled_data)\n",
    "        \n",
    "    # --- Step 8 ---\n",
    "    visualize_comparison(data_pca, all_labels, all_metrics, feature_df)\n",
    "    \n",
    "    # --- Step 9 ---\n",
    "    # Create some mock data for a new, very stable coin\n",
    "    # [timestamp, open, high, low, close, volume]\n",
    "    mock_new_data = [\n",
    "        [1678886400000, 1.00, 1.01, 1.00, 1.01, 100000],\n",
    "        [1678972800000, 1.01, 1.01, 1.00, 1.00, 120000],\n",
    "        [1679059200000, 1.00, 1.02, 1.00, 1.01, 110000],\n",
    "        [1679145600000, 1.01, 1.01, 1.01, 1.01, 90000]\n",
    "    ] # ... and so on for 365 days.\n",
    "    \n",
    "    # For this example, let's just use data from a real coin\n",
    "    # to prove the function works. We'll use BTC's data.\n",
    "    print(\"\\n--- Testing Prediction Function (using BTC data as a 'new' coin) ---\")\n",
    "    btc_raw_data = raw_data_dict['BTC/USDT'][['timestamp', 'open', 'high', 'low', 'close', 'volume']].values.tolist()\n",
    "    \n",
    "    predicted_cluster = predict_new_crypto(\n",
    "        new_ohlcv_data=btc_raw_data,\n",
    "        scaler=data_scaler,\n",
    "        kmeans_model=models['kmeans']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nThe 'new' crypto (BTC data) was placed in K-Means cluster: {predicted_cluster}\")\n",
    "    \n",
    "    # Let's see which cluster BTC was *actually* in\n",
    "    btc_original_cluster = all_labels['kmeans'][feature_df.index.get_loc('BTC/USDT')]\n",
    "    print(f\"The original BTC was in cluster: {btc_original_cluster}\")\n",
    "    \n",
    "    if predicted_cluster == btc_original_cluster:\n",
    "        print(\"Success! The prediction function correctly clustered the new data.\")\n",
    "    else:\n",
    "        print(\"Note: The cluster is different. This can happen due to minor data differences.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
